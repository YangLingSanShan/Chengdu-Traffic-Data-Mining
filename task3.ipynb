{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertConfig, BertModel\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./product_data/task3_todo.csv already exists. Skipping processing.\n",
      "2018-09-30 15:50:58+00:00\n"
     ]
    }
   ],
   "source": [
    "def process_data(file_path, output_path):\n",
    "    # 读取数据\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    base_time = df['time'].min()\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"{output_path} already exists. Skipping processing.\")\n",
    "        return base_time\n",
    "\n",
    "\n",
    "    df['time_offset'] = (df['time'] - base_time).dt.total_seconds()\n",
    "\n",
    "    df[['longitude', 'latitude']] = df['coordinates'].apply(lambda x: eval(x) if pd.notna(x) else [None, None]).tolist()\n",
    "    \n",
    "    # 转换\n",
    "    df[['longitude', 'latitude']] = df.apply(lambda row: pd.Series(tbd.gcj02towgs84(row['longitude'], row['latitude'])), axis=1)\n",
    "\n",
    "    df['holiday'] = df['time'].apply(lambda x: 1 if (x.month == 10 and 1 <= x.day <= 7) else 0)\n",
    "\n",
    "    weather_df = pd.read_csv('./data/weather.csv')\n",
    "\n",
    "\n",
    "    weather_df['Date'] = pd.to_datetime(weather_df['Date'])\n",
    "    df['date'] = df['time'].dt.date\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # 合并轨迹数据和天气数据\n",
    "    df = pd.merge(df, weather_df, left_on='date', right_on='Date', how='left')\n",
    "\n",
    "    df.drop(columns=['Date','Day','date'], inplace=True)\n",
    "\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # 保存处理后的数据\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return base_time\n",
    "\n",
    "base_time = process_data('./data/eta_task.csv', './product_data/task3_todo.csv')\n",
    "print(base_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./product_data/task3_todo.csv')\n",
    "\n",
    "cols_to_fill = ['High Temp', 'Low Temp', 'Rain', 'Wind Force']\n",
    "\n",
    "ref_df = df.groupby('trajectory_id')[cols_to_fill].first().reset_index()\n",
    "holiday_ref_df = df.groupby('trajectory_id')['holiday'].first().reset_index()\n",
    "\n",
    "merged = pd.merge(df, ref_df, on='trajectory_id', suffixes=('', '_ref'))\n",
    "merged = pd.merge(merged, holiday_ref_df, on='trajectory_id', suffixes=('', '_holiday_ref'))\n",
    "\n",
    "for col in cols_to_fill:\n",
    "    merged[col] = merged[col].fillna(merged[col + '_ref'])\n",
    "\n",
    "merged['holiday'] = merged['holiday_holiday_ref']\n",
    "\n",
    "cols_to_drop = [col + '_ref' for col in cols_to_fill] + ['holiday_holiday_ref']\n",
    "merged.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "df_filled = merged\n",
    "df_filled.to_csv('./product_data/task3_todo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "df = pd.read_csv('./product_data/task4_train_data.csv') # revise after task4\n",
    "\n",
    "df = df.drop(['type', 'time', 'coordinates'], axis=1)\n",
    "\n",
    "unique_ids = df['trajectory_id'].unique()\n",
    "\n",
    "train_ids, test_ids = train_test_split(\n",
    "    unique_ids,\n",
    "    test_size=0.2,\n",
    "    random_state=114514\n",
    ")\n",
    "\n",
    "train_df = df[df['trajectory_id'].isin(train_ids)].copy()\n",
    "test_df = df[df['trajectory_id'].isin(test_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols = ['longitude', 'latitude']\n",
    "\n",
    "discrete_cols = ['holiday','Rain', 'High Temp', 'Low Temp', 'Wind Force']\n",
    "\n",
    "target_col = ['time_offset']\n",
    "\n",
    "# 计算训练集的均值和标准差\n",
    "target_mean = train_df['time_offset'].mean()\n",
    "target_std = train_df['time_offset'].std()\n",
    "\n",
    "# 使用训练集的参数标准化训练集\n",
    "train_df['time_offset'] = (train_df['time_offset'] - target_mean) / target_std\n",
    "\n",
    "# 使用相同的参数标准化测试集\n",
    "test_df['time_offset'] = (test_df['time_offset'] - target_mean) / target_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化连续特征\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 拟合并转换训练集\n",
    "train_df[continuous_cols] = scaler.fit_transform(train_df[continuous_cols])\n",
    "\n",
    "# 仅转换测试集\n",
    "test_df[continuous_cols] = scaler.transform(test_df[continuous_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码离散特征\n",
    "label_encoders = {}\n",
    "for col in discrete_cols:\n",
    "    le = LabelEncoder()\n",
    "    train_df[col] = le.fit_transform(train_df[col])\n",
    "    test_df[col] = le.transform(test_df[col])\n",
    "    label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据按trajectory_id分组\n",
    "def group_by_trajectory(df):\n",
    "    grouped = df.groupby('trajectory_id')\n",
    "    sequences = []\n",
    "    for _, group in grouped:\n",
    "        group_data = group.sort_values(by='trajectory_id')\n",
    "        sequences.append({\n",
    "            'features': group[continuous_cols + discrete_cols].values,\n",
    "            'target': group[target_col].values\n",
    "        })\n",
    "    return sequences\n",
    "\n",
    "train_sequences = group_by_trajectory(train_df)\n",
    "test_sequences = group_by_trajectory(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_len = 30 \n",
    "max_decoder_len = 15 \n",
    "\n",
    "def process_sequences(sequences, max_enc_len, max_dec_len):\n",
    "    encoder_inputs = []\n",
    "    decoder_inputs = []\n",
    "    targets = []\n",
    "    encoder_masks = []\n",
    "    decoder_masks = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        features = seq['features']  # [seq_len, feature_dim]\n",
    "        target = seq['target']      # [seq_len, 1]\n",
    "\n",
    "        enc_input = features[:max_enc_len]\n",
    "        encoder_inputs.append(torch.tensor(enc_input, dtype=torch.float32))\n",
    "\n",
    "        dec_input = features[:max_dec_len]  \n",
    "        decoder_inputs.append(torch.tensor(dec_input, dtype=torch.float32))\n",
    "\n",
    "        target_seq = target[:max_dec_len]\n",
    "        targets.append(torch.tensor(target_seq, dtype=torch.float32))\n",
    "\n",
    "        enc_mask = torch.zeros(max_enc_len, dtype=torch.int64)\n",
    "        enc_mask[:len(enc_input)] = 1\n",
    "        encoder_masks.append(enc_mask)\n",
    "\n",
    "        dec_mask = torch.zeros(max_dec_len, dtype=torch.int64)\n",
    "        dec_mask[:len(dec_input)] = 1\n",
    "        decoder_masks.append(dec_mask)\n",
    "\n",
    "    encoder_inputs = pad_sequence(encoder_inputs, batch_first=True, padding_value=0.0)  # [B, T_enc, F]\n",
    "    decoder_inputs = pad_sequence(decoder_inputs, batch_first=True, padding_value=0.0)  # [B, T_dec, F]\n",
    "    targets = pad_sequence(targets, batch_first=True, padding_value=0.0)                # [B, T_dec]\n",
    "\n",
    "    encoder_masks = torch.stack(encoder_masks)  # [B, T_enc]\n",
    "    decoder_masks = torch.stack(decoder_masks)  # [B, T_dec]\n",
    "\n",
    "    return encoder_inputs, decoder_inputs, targets, encoder_masks, decoder_masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, encoder_inputs, decoder_inputs, targets, encoder_masks, decoder_masks):\n",
    "        self.encoder_inputs = encoder_inputs  # Encoder输入\n",
    "        self.decoder_inputs = decoder_inputs  # Decoder输入 (通常是目标序列的历史值)\n",
    "        self.targets = targets                # 目标序列\n",
    "        self.encoder_masks = encoder_masks    # Encoder mask\n",
    "        self.decoder_masks = decoder_masks    # Decoder mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoder_inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'encoder_inputs': self.encoder_inputs[idx].clone().detach().to(dtype=torch.float32),\n",
    "            'decoder_inputs': self.decoder_inputs[idx].clone().detach().to(dtype=torch.float32),\n",
    "            'targets': self.targets[idx].clone().detach().to(dtype=torch.float32),\n",
    "            'encoder_masks': self.encoder_masks[idx].clone().detach().to(dtype=torch.int64),\n",
    "            'decoder_masks': self.decoder_masks[idx].clone().detach().to(dtype=torch.int64)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_EncoderDecoder_2(nn.Module):\n",
    "    def __init__(self, feature_dim, d_model=128, num_layers=6, num_heads=4):\n",
    "        super(Transformer_EncoderDecoder_2, self).__init__()\n",
    "        # Encoder配置\n",
    "        self.encoder_config = BertConfig(\n",
    "            hidden_size=d_model,\n",
    "            num_hidden_layers=num_layers,\n",
    "            num_attention_heads=num_heads,\n",
    "            intermediate_size=4 * d_model,\n",
    "            max_position_embeddings=128,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.05\n",
    "        )\n",
    "        self.encoder = BertModel(self.encoder_config)\n",
    "\n",
    "        # Decoder配置\n",
    "        self.decoder_config = BertConfig(\n",
    "            hidden_size=d_model,\n",
    "            num_hidden_layers=num_layers,\n",
    "            num_attention_heads=num_heads,\n",
    "            intermediate_size=4 * d_model,\n",
    "            max_position_embeddings=128,\n",
    "            hidden_dropout_prob=0.03,\n",
    "            attention_probs_dropout_prob=0.05,\n",
    "            is_decoder=True,\n",
    "            add_cross_attention=True  # 启用交叉注意力\n",
    "        )\n",
    "        self.decoder = BertModel(self.decoder_config)\n",
    "\n",
    "        # 输入映射到 Transformer 的 d_model\n",
    "        self.input_projection = nn.Linear(feature_dim, d_model)\n",
    "\n",
    "        # 输出预测层\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, encoder_inputs, decoder_inputs, encoder_masks, decoder_masks):\n",
    "        # Encoder阶段\n",
    "        encoder_inputs = self.input_projection(encoder_inputs)  # [batch_size, seq_len, d_model]\n",
    "        encoder_outputs = self.encoder(inputs_embeds=encoder_inputs, attention_mask=encoder_masks)\n",
    "\n",
    "        # Decoder阶段\n",
    "        decoder_inputs = self.input_projection(decoder_inputs)  # [batch_size, seq_len, d_model]\n",
    "        decoder_outputs = self.decoder(\n",
    "            inputs_embeds=decoder_inputs,\n",
    "            attention_mask=decoder_masks,\n",
    "            encoder_hidden_states=encoder_outputs.last_hidden_state,\n",
    "            encoder_attention_mask=encoder_masks\n",
    "        )\n",
    "\n",
    "        # 输出预测\n",
    "        increments_logits = self.fc(decoder_outputs.last_hidden_state)\n",
    "        # （2）确保增量为正\n",
    "        increments = F.softplus(increments_logits) + 1e-9\n",
    "        # （3）在 seq_len 维度上做累加\n",
    "        predictions_cumsum = torch.cumsum(increments, dim=1) \n",
    "        \n",
    "        # 返回去掉最后一维的结果 => [batch_size, seq_len]\n",
    "        return predictions_cumsum.squeeze(-1)\n",
    "    \n",
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self, feature_dim, d_model=128, num_layers=6, num_heads=8):\n",
    "        super(TransformerEncoderDecoder, self).__init__()\n",
    "        # Encoder配置\n",
    "        self.encoder_config = BertConfig(\n",
    "            hidden_size=d_model,\n",
    "            num_hidden_layers=num_layers,\n",
    "            num_attention_heads=num_heads,\n",
    "            intermediate_size=4 * d_model,\n",
    "            max_position_embeddings=512,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1\n",
    "        )\n",
    "        self.encoder = BertModel(self.encoder_config)\n",
    "\n",
    "        # Decoder配置\n",
    "        self.decoder_config = BertConfig(\n",
    "            hidden_size=d_model,\n",
    "            num_hidden_layers=num_layers,\n",
    "            num_attention_heads=num_heads,\n",
    "            intermediate_size=4 * d_model,\n",
    "            max_position_embeddings=512,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1,\n",
    "            is_decoder=True,\n",
    "            add_cross_attention=True  # 启用交叉注意力\n",
    "        )\n",
    "        self.decoder = BertModel(self.decoder_config)\n",
    "\n",
    "        # 输入映射到 Transformer 的 d_model\n",
    "        self.input_projection = nn.Linear(feature_dim, d_model)\n",
    "\n",
    "        # 输出预测层\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, encoder_inputs, decoder_inputs, encoder_masks, decoder_masks):\n",
    "        # Encoder阶段\n",
    "        encoder_inputs = self.input_projection(encoder_inputs)  # [batch_size, seq_len, d_model]\n",
    "        encoder_outputs = self.encoder(inputs_embeds=encoder_inputs, attention_mask=encoder_masks)\n",
    "\n",
    "        # Decoder阶段\n",
    "        decoder_inputs = self.input_projection(decoder_inputs)  # [batch_size, seq_len, d_model]\n",
    "        decoder_outputs = self.decoder(\n",
    "            inputs_embeds=decoder_inputs,\n",
    "            attention_mask=decoder_masks,\n",
    "            encoder_hidden_states=encoder_outputs.last_hidden_state,\n",
    "            encoder_attention_mask=encoder_masks\n",
    "        )\n",
    "\n",
    "        # 输出预测\n",
    "        predictions = self.fc(decoder_outputs.last_hidden_state)  # [batch_size, seq_len, 1]\n",
    "        return predictions.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_to_hour(data,predictions_list,i):\n",
    "    temp = data.groupby('trajectory_id').first()\n",
    "    temp = pd.to_datetime(temp['time'])\n",
    "\n",
    "    temp_hour = temp.dt.hour.tolist()\n",
    "    temp_minute = temp.dt.minute.tolist()\n",
    "\n",
    "    # 将预测值看作秒数，加到基准时间 base_time\n",
    "    todo = base_time + pd.Timedelta(seconds=float(predictions_list[0]))\n",
    "\n",
    "    # 让输出时间的小时固定为和 ref_time 相同\n",
    "    new_hour = temp_hour[i]\n",
    "\n",
    "    # 如果输出时间的分钟大于 ref_time 的分钟，则加一小时\n",
    "    if todo.minute > temp_minute[i] + 10:\n",
    "        new_hour += 1\n",
    "\n",
    "    # 用 replace 更新小时\n",
    "    # 注意：replace 不改变日期，但会改变小时，可能需要考虑跨日情况\n",
    "    # 如果 new_hour >= 24，需要额外加一天，这里仅作简单示例\n",
    "    if new_hour >= 24:\n",
    "        # 加一天，并让小时在 0-23 之间\n",
    "        todo = todo + pd.Timedelta(days=1)\n",
    "        new_hour = new_hour % 24\n",
    "\n",
    "    todo = todo.replace(hour=new_hour)\n",
    "\n",
    "    # 最后输出你想要的格式\n",
    "    time_str = todo.strftime('%Y-%m-%d %H:%M:%S.%f%z')\n",
    "    return time_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(preds, targets, lambda_=0.07):\n",
    "    \"\"\"\n",
    "    preds:  形状 [batch_size, seq_len], 模型输出的预测序列\n",
    "    targets:[batch_size, seq_len], 真实标签\n",
    "    lambda_: 惩罚系数\n",
    "    \"\"\"\n",
    "    # 1) mae\n",
    "    base_loss = F.mse_loss(preds, targets)\n",
    "    # 2) 惩罚项：\n",
    "    increments = preds[:, 1:] - preds[:, :-1]  # [batch_size, seq_len-1]\n",
    "    \n",
    "    \n",
    "    penalty = F.relu(increments - 1e-7)        \n",
    "    \n",
    "    penalty_value = penalty.sum()\n",
    "\n",
    "    # 3) 合并最终损失\n",
    "    total_loss = base_loss + lambda_ * penalty_value\n",
    "    \n",
    "    return total_loss, penalty_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs, decoder_inputs, targets, encoder_masks, decoder_masks = process_sequences(\n",
    "    train_sequences, max_encoder_len, max_decoder_len\n",
    ")\n",
    "test_encoder_inputs, test_decoder_inputs, test_targets, test_encoder_masks, test_decoder_masks = process_sequences(\n",
    "    test_sequences, max_encoder_len, max_decoder_len\n",
    ")\n",
    "\n",
    "dataset = TimeSeriesDataset(encoder_inputs, decoder_inputs, targets, encoder_masks, decoder_masks)\n",
    "test_dataset = TimeSeriesDataset(test_encoder_inputs, test_decoder_inputs, test_targets, test_encoder_masks, test_decoder_masks)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 初始化模型\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerEncoderDecoder(feature_dim=7, d_model=128, num_layers=2, num_heads=4).to(device)\n",
    "epochs = 100\n",
    "evaluation_interval = 5\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs / 4, eta_min=2e-6)\n",
    "\n",
    "train_losses = []\n",
    "test_rmse = []\n",
    "test_mae = []\n",
    "evaluation_epochs = []\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "def compute_rmse(preds, targets):\n",
    "    return torch.sqrt(nn.MSELoss()(preds, targets)).item()\n",
    "\n",
    "# Function to compute MAE\n",
    "def compute_mae(preds, targets):\n",
    "    return nn.L1Loss()(preds, targets).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  68%|██████▊   | 226/333 [00:05<00:02, 41.41batch/s, loss=0.998]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(encoder_inputs, decoder_inputs, encoder_masks, decoder_masks)\n\u001b[0;32m     15\u001b[0m loss, penalty \u001b[38;5;241m=\u001b[39m custom_loss(predictions, targets, lambda_\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.07\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     19\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    with tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\") as progress_bar:\n",
    "        for batch in progress_bar:\n",
    "            encoder_inputs = batch['encoder_inputs'].to(device)\n",
    "            decoder_inputs = batch['decoder_inputs'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            targets = targets.squeeze(-1)  # Remove the last dimension\n",
    "            encoder_masks = batch['encoder_masks'].to(device)\n",
    "            decoder_masks = batch['decoder_masks'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(encoder_inputs, decoder_inputs, encoder_masks, decoder_masks)\n",
    "            \n",
    "            loss, penalty = custom_loss(predictions, targets, lambda_=0.07)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = epoch_loss / len(dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}, Penalty: {penalty:.7f}\")\n",
    "\n",
    "    if (epoch + 1) % evaluation_interval == 0:\n",
    "        model.eval()  \n",
    "        with torch.no_grad():\n",
    "            total_rmse = 0.0\n",
    "            total_mae = 0.0\n",
    "            total_samples = 0\n",
    "\n",
    "            for batch in test_dataloader:\n",
    "                encoder_inputs = batch['encoder_inputs'].to(device)\n",
    "                decoder_inputs = batch['decoder_inputs'].to(device)\n",
    "                targets = batch['targets'].to(device)\n",
    "                targets = targets.squeeze(-1)\n",
    "                encoder_masks = batch['encoder_masks'].to(device)\n",
    "                decoder_masks = batch['decoder_masks'].to(device)\n",
    "\n",
    "                predictions = model(encoder_inputs, decoder_inputs, encoder_masks, decoder_masks)\n",
    "\n",
    "                predictions = predictions.view(-1)\n",
    "\n",
    "                targets = targets.view(-1)\n",
    "\n",
    "                rmse = compute_rmse(predictions, targets)\n",
    "                mae = compute_mae(predictions, targets)\n",
    "\n",
    "                total_rmse += rmse * targets.size(0)\n",
    "                total_mae += mae * targets.size(0)\n",
    "                total_samples += targets.size(0)\n",
    "\n",
    "            avg_rmse = total_rmse / total_samples\n",
    "            avg_mae = total_mae / total_samples\n",
    "\n",
    "            test_rmse.append(avg_rmse)\n",
    "            test_mae.append(avg_mae)\n",
    "            evaluation_epochs.append(epoch + 1)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            print(f\"--- Evaluation after Epoch {epoch + 1} ---\")\n",
    "            print(f\"Test RMSE: {avg_rmse:.4f}, Test MAE: {avg_mae:.4f}, Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        model.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(evaluation_epochs, test_rmse, label='Test RMSE', marker='o')\n",
    "plt.plot(evaluation_epochs, test_mae, label='Test MAE', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Test Metrics Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "torch.save(model.state_dict(), './models/task3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 已成功输出到 'output.csv' 文件中\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('./models/3.pth')\n",
    "model = TransformerEncoderDecoder(feature_dim=7, d_model=128, num_layers=3, num_heads=4).to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "df = pd.read_csv('./product_data/task3_todo.csv')\n",
    "df[continuous_cols] = scaler.transform(df[continuous_cols])\n",
    "\n",
    "discrete_cols = ['holiday','Rain', 'High Temp', 'Low Temp', 'Wind Force']\n",
    "for col in discrete_cols:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "fill_sequences = group_by_trajectory(df)\n",
    "\n",
    "fill_encoder_inputs, fill_decoder_inputs, fill_targets, fill_encoder_masks, fill_decoder_masks = process_sequences(\n",
    "    fill_sequences, max_encoder_len, max_decoder_len\n",
    ")\n",
    "\n",
    "fill_dataset = TimeSeriesDataset(fill_encoder_inputs, fill_decoder_inputs, fill_targets, fill_encoder_masks, fill_decoder_masks)\n",
    "fill_dataloader = DataLoader(fill_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "j = 0\n",
    "times = []\n",
    "ids = []\n",
    "for batch in fill_dataloader:\n",
    "    encoder_inputs = batch['encoder_inputs'].to(device)\n",
    "    decoder_inputs = batch['decoder_inputs'].to(device)\n",
    "    targets = batch['targets'].to(device).squeeze(-1)\n",
    "    encoder_masks = batch['encoder_masks'].to(device)\n",
    "    decoder_masks = batch['decoder_masks'].to(device)\n",
    "\n",
    "    # 模型预测\n",
    "    predictions = model(encoder_inputs, decoder_inputs, encoder_masks, decoder_masks)\n",
    "    predictions = predictions * target_std + target_mean\n",
    "    predictions_lists = predictions.tolist()\n",
    "\n",
    "    # 遍历每条预测\n",
    "    id = 26628 + j\n",
    "    for i in range(len(predictions_lists)): \n",
    "        times.append(offset_to_hour(df, predictions_lists[i], i))\n",
    "        ids.append(id)\n",
    "    j += 1\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'time': times,\n",
    "})\n",
    "\n",
    "df.to_csv('./results/task3_results.csv', index=False)\n",
    "\n",
    "print(\"DataFrame 已成功输出到 'output.csv' 文件中\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
